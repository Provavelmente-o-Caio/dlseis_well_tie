{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import segyio\n",
    "import lasio\n",
    "import yaml\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from wtie import grid, viz, autotie\n",
    "from wtie.processing.logs import interpolate_nans, despike\n",
    "from wtie.utils.datasets import tutorial\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pathfolder = Path('.'))\n",
    "seis_path_3d_path = Path(\n",
    "    \"c:\\\\Users\\\\Caio\\\\Downloads\\\\Rawdata\\\\Rawdata\\\\Seismic_data.sgy\"\n",
    ")\n",
    "seis_path_2d_path = Path(\n",
    "    \"../data/tutorial/Poseidon/boreas1/Boreas1_seismic_alongwell_0_0.sgy\"\n",
    ")\n",
    "\n",
    "las_log_3d_path = Path(\n",
    "    \"c:\\\\Users\\\\Caio\\\\Downloads\\\\Rawdata\\\\Rawdata\\\\Well_data\\\\F02-01_logs.las\"\n",
    ")\n",
    "las_log_2d_path = Path(\"../data/tutorial/Poseidon/boreas1/Boreas1Decim.las\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with segyio.open(seis_path_2d_path, \"r\") as f:\n",
    "    print(f.samples.size)  # number of time samples\n",
    "    print(f.ilines)\n",
    "    print(f.xlines)\n",
    "    print(f.offsets)  # these are actually angles, from 0 to 45 degrees\n",
    "    print(f.format)\n",
    "    print(f.header)  # this is the header of the file\n",
    "\n",
    "with segyio.open(seis_path_3d_path, \"r\", ignore_geometry=True) as f:\n",
    "    print(f.samples.size)  # number of time samples\n",
    "    print(f.ilines)\n",
    "    print(f.xlines)\n",
    "    print(f.offsets)  # these are actually angles, from 0 to 45 degrees\n",
    "    print(f.format)\n",
    "    print(f.header)  # this is the header of the file\n",
    "    ilines = [f.header[i][segyio.TraceField.INLINE_3D] for i in range(f.tracecount)]\n",
    "    xlines = [f.header[i][segyio.TraceField.CROSSLINE_3D] for i in range(f.tracecount)]\n",
    "    print(set(ilines))\n",
    "    print(set(xlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_segy_format(segy_path):\n",
    "    with segyio.open(segy_path, \"r\", ignore_geometry=True) as f:\n",
    "        ilines = [f.header[i][segyio.TraceField.INLINE_3D] for i in range(f.tracecount)]\n",
    "        xlines = [\n",
    "            f.header[i][segyio.TraceField.CROSSLINE_3D] for i in range(f.tracecount)\n",
    "        ]\n",
    "        n_ilines = len(set(ilines))\n",
    "        n_xlines = len(set(xlines))\n",
    "        if n_ilines == 1 or n_xlines == 1:\n",
    "            return \"2D\"\n",
    "        else:\n",
    "            return \"3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_log_3d = lasio.read(las_log_3d_path)\n",
    "las_log_3d = las_log_3d.df()\n",
    "\n",
    "las_log_2d = lasio.read(las_log_2d_path)\n",
    "las_log_2d = las_log_2d.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_logs_generic(file_path: str) -> grid.LogSet:\n",
    "    \"\"\"\n",
    "    Importa qualquer arquivo LAS e retorna um grid.LogSet com todos os logs disponíveis,\n",
    "    mapeando nomes alternativos para 'Vp' e 'Rho' se necessário e interpolando para base regular.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    las_logs = lasio.read(file_path)\n",
    "    las_df = las_logs.df()\n",
    "    logs = {}\n",
    "    # Define base regular (usando o menor passo encontrado)\n",
    "    md = las_df.index.values\n",
    "    step = np.min(np.diff(md))\n",
    "    md_regular = np.arange(md[0], md[-1] + step, step)\n",
    "    # Mapeamento de nomes alternativos\n",
    "    name_map = {\n",
    "        \"Vp\": [\n",
    "            col\n",
    "            for col in las_df.columns\n",
    "            if col.startswith(\"DT\") or col.upper() in [\"VP\", \"DTCO\", \"SONIC\"]\n",
    "        ],\n",
    "        \"Rho\": [\n",
    "            col\n",
    "            for col in las_df.columns\n",
    "            if col.upper() in [\"RHO\", \"RHOB\", \"RHOZ\", \"DENS\"]\n",
    "        ],\n",
    "    }\n",
    "    # Primeiro, tenta mapear os obrigatórios\n",
    "    for key, aliases in name_map.items():\n",
    "        for alias in aliases:\n",
    "            if alias in las_df.columns:\n",
    "                values = las_df[alias].values\n",
    "                # Interpola para base regular\n",
    "                interp_values = np.interp(md_regular, md, interpolate_nans(values))\n",
    "                if key == \"Vp\":\n",
    "                    if np.nanmean(values) > 100:  # provavelmente DT\n",
    "                        interp_values = 1 / interp_values * 1e6 / 3.2808  # ft/us -> m/s\n",
    "                logs[key] = grid.Log(interp_values, md_regular, \"md\", name=key)\n",
    "                break\n",
    "    # Adiciona os demais logs\n",
    "    for col in las_df.columns:\n",
    "        if col not in logs:\n",
    "            try:\n",
    "                values = las_df[col].values\n",
    "                interp_values = np.interp(md_regular, md, interpolate_nans(values))\n",
    "                logs[col] = grid.Log(interp_values, md_regular, \"md\", name=col)\n",
    "            except Exception as e:\n",
    "                print(f\"Não foi possível importar o log {col}: {e}\")\n",
    "    return grid.LogSet(logs=logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_well_path_generic(file_path: str, kb: float = 0) -> grid.WellPath:\n",
    "    \"\"\"\n",
    "    Importa um arquivo de trajetória de poço.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    try:\n",
    "        # Verifica primeiro se é formato Boreas (6 colunas)\n",
    "        with open(file_path) as f:\n",
    "            # Pula a primeira linha\n",
    "            next(f)\n",
    "            primeira_linha_dados = next(f).strip()\n",
    "\n",
    "        # Se tem 6 números, é formato Boreas\n",
    "        if len(primeira_linha_dados.split()) == 11:\n",
    "            # Lê pulando apenas as duas primeiras linhas\n",
    "            df = pd.read_csv(file_path, header=None, delimiter=r\"\\s+\", skiprows=[0, 1])\n",
    "            # Reorganiza os dados das 6 colunas em 3\n",
    "            data = []\n",
    "            for _, row in df.iterrows():\n",
    "                data.extend([(row[0], row[1]), (row[4], row[6])])\n",
    "            df_clean = pd.DataFrame(data, columns=[\"MD\", \"INC\"])\n",
    "        else:\n",
    "            # Formato Volve (3 colunas)\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                header=None,\n",
    "                delimiter=r\"\\s+\",\n",
    "                comment=\"#\",\n",
    "                skip_blank_lines=True,\n",
    "            )\n",
    "            df.columns = [\"MD\", \"INC\", \"AZI\"][: len(df.columns)]\n",
    "            df_clean = df[[\"MD\", \"INC\"]]\n",
    "\n",
    "        # Limpa e ordena os dados\n",
    "        df_clean = df_clean.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "        df_clean = df_clean.sort_values(\"MD\").drop_duplicates(\"MD\")\n",
    "\n",
    "        md = df_clean[\"MD\"].values\n",
    "        inc = df_clean[\"INC\"].values\n",
    "\n",
    "        # Para o formato Volve, garantimos que inc tem um elemento a menos que md\n",
    "        if len(inc) == len(md):\n",
    "            inc = inc[:-1]\n",
    "\n",
    "        # Garante que começa em MD=0\n",
    "        if md[0] != 0:\n",
    "            md = np.concatenate(([0.0], md))\n",
    "            inc = np.concatenate(([0.0], inc))\n",
    "\n",
    "        # Garante que os valores são estritamente crescentes\n",
    "        mask = np.diff(md) > 0\n",
    "        mask = np.concatenate(([True], mask))\n",
    "        md = md[mask]\n",
    "        inc = inc[mask[:-1]]  # ajusta o tamanho de inc\n",
    "\n",
    "        # Calcula TVD e converte para TVDSS\n",
    "        tvd = grid.WellPath.get_tvdkb_from_inclination(md, inc)\n",
    "        tvd = grid.WellPath.tvdkb_to_tvdss(tvd, kb)\n",
    "\n",
    "        return grid.WellPath(md=md, tvdss=tvd, kb=kb)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erro ao ler arquivo {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_time_depth_table_generic(file_path: str) -> grid.TimeDepthTable:\n",
    "    \"\"\"\n",
    "    Importa uma tabela tempo/profundidade genérica (espera colunas: TWT, TVDSS).\n",
    "    Suporta formatos:\n",
    "    1. Duas colunas simples: TWT TVDSS\n",
    "    2. Formato Boreas: Depth TVDSS OWT Depth TVDSS OWT\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    # Lê o arquivo ignorando comentários\n",
    "    df = pd.read_csv(\n",
    "        file_path, delimiter=r\"\\s+\", header=None, comment=\"#\", skip_blank_lines=True\n",
    "    )\n",
    "\n",
    "    # Verifica o formato pelo número de colunas\n",
    "    if len(df.columns) >= 6:  # Formato Boreas\n",
    "        # Reorganiza os dados das 6 colunas em 2 (TVDSS, OWT)\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Pega apenas TVDSS e OWT de cada par\n",
    "            data.extend([(row[2], row[1]), (row[5], row[4])])\n",
    "        df_clean = pd.DataFrame(data, columns=[\"TWT\", \"TVDSS\"])\n",
    "    else:  # Formato duas colunas\n",
    "        df_clean = df.iloc[:, :2]\n",
    "        df_clean.columns = [\"TWT\", \"TVDSS\"]\n",
    "\n",
    "    # Limpa e ordena os dados\n",
    "    df_clean = df_clean.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "    df_clean = df_clean.sort_values(\"TWT\").drop_duplicates(\"TWT\")\n",
    "\n",
    "    # Extrai os arrays\n",
    "    twt = df_clean[\"TWT\"].values\n",
    "    tvdss = df_clean[\"TVDSS\"].values\n",
    "\n",
    "    # OWT para TWT (se necessário)\n",
    "    if np.mean(twt) < 1:  # provavelmente OWT\n",
    "        twt = 2 * twt  # converte para TWT\n",
    "\n",
    "    return grid.TimeDepthTable(twt=twt, tvdss=tvdss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_seismic_trace(file_path: str, trace_idx: int = 0) -> grid.Seismic:\n",
    "    \"\"\"\n",
    "    Importa um arquivo SEGY e retorna apenas um traço específico como grid.Seismic.\n",
    "    TODO: Melhorar a importação para lidar com diferentes formatos de SEGY (1D/2D/3D).\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    with segyio.open(file_path, \"r\", ignore_geometry=True) as f:\n",
    "        twt = f.samples / 1000  # ms para s\n",
    "        trace = f.trace[trace_idx]  # pega o traço desejado\n",
    "    return grid.Seismic(trace, twt, \"twt\", name=f\"{file_path.stem}_trace{trace_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wtie.utils.datasets.utils import InputSet\n",
    "\n",
    "log_path = \"../data/tutorial/Poseidon/boreas1/Boreas1Decim.las\"\n",
    "seismic_path = \"../data/tutorial/Poseidon/boreas1/Boreas1_seismic_alongwell_0_0.sgy\"\n",
    "wellpath_path = \"../data/tutorial/Poseidon/Boreas1/Boreas1_dev.txt\"\n",
    "td_table_path = \"../data/tutorial/Poseidon/boreas1/Boreas1_vel.txt\"\n",
    "\n",
    "logset_md = import_logs_generic(log_path)\n",
    "seismic = import_seismic_trace(seismic_path)\n",
    "wellpath = import_well_path_generic(wellpath_path)\n",
    "td_table = import_time_depth_table_generic(td_table_path)\n",
    "\n",
    "inputs = InputSet(\n",
    "    logset_md=logset_md, seismic=seismic, wellpath=wellpath, table=td_table\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = Path(\"../data/tutorial/trained_net_state_dict.pt\")\n",
    "assert model_state_dict.is_file()\n",
    "\n",
    "with open(Path(\"../data/tutorial/network_parameters.yaml\"), \"r\") as yaml_file:\n",
    "    training_parameters = yaml.load(yaml_file, Loader=yaml.Loader)\n",
    "\n",
    "wavelet_extractor = tutorial.load_wavelet_extractor(\n",
    "    training_parameters, model_state_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler = tutorial.get_modeling_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autotie.tie_v1.__doc__)\n",
    "print(autotie.get_default_search_space_v1.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_length_choice = dict(\n",
    "    name=\"logs_median_size\",\n",
    "    type=\"choice\",\n",
    "    values=[i for i in range(11, 63, 2)],\n",
    "    value_type=\"int\",\n",
    ")\n",
    "\n",
    "median_th_choice = dict(\n",
    "    name=\"logs_median_threshold\", type=\"range\", bounds=[0.1, 5.5], value_type=\"float\"\n",
    ")\n",
    "\n",
    "std_choice = dict(name=\"logs_std\", type=\"range\", bounds=[0.5, 5.5], value_type=\"float\")\n",
    "\n",
    "table_t_shift_choice = dict(\n",
    "    name=\"table_t_shift\", type=\"range\", bounds=[-0.012, 0.012], value_type=\"float\"\n",
    ")\n",
    "\n",
    "search_space = [\n",
    "    median_length_choice,\n",
    "    median_th_choice,\n",
    "    std_choice,\n",
    "    table_t_shift_choice,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = dict(num_iters=80, similarity_std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_scaling_params = dict(\n",
    "    wavelet_min_scale=50000, wavelet_max_scale=500000, num_iters=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "outputs = autotie.tie_v1(\n",
    "    inputs,\n",
    "    wavelet_extractor,\n",
    "    modeler,\n",
    "    wavelet_scaling_params,\n",
    "    search_params=search_params,\n",
    "    search_space=search_space,\n",
    "    stretch_and_squeeze_params=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values = outputs.ax_client.get_best_parameters()\n",
    "means, covariances = values\n",
    "print(means)\n",
    "print(covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.plot_optimization_objective();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = outputs.plot_wavelet(fmax=85, phi_max=15, figsize=(6, 5))\n",
    "axes[0].set_xlim((-0.1, 0.1))\n",
    "axes[2].set_ylim((-12.5, 12.5))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_scale = 120000\n",
    "fig, axes = outputs.plot_tie_window(wiggle_scale=_scale, figsize=(12.0, 7.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = viz.plot_td_table(inputs.table, plot_params=dict(label=\"original\"))\n",
    "viz.plot_td_table(outputs.table, plot_params=dict(label=\"modified\"), fig_axes=(fig, ax))\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_and_s_params = dict(window_length=0.060, max_lag=0.010)  # in seconds\n",
    "\n",
    "outputs2 = autotie.stretch_and_squeeze(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    wavelet_extractor,\n",
    "    modeler,\n",
    "    wavelet_scaling_params,\n",
    "    best_parameters,\n",
    "    s_and_s_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = outputs2.plot_wavelet(fmax=85, phi_max=25, figsize=(6, 5))\n",
    "axes[0].set_xlim((-0.1, 0.1))\n",
    "axes[2].set_ylim((-6.0, 12.5))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = outputs2.plot_tie_window(wiggle_scale=_scale, figsize=(12.0, 7.5))\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "axes[0].xaxis.set_major_locator(MaxNLocator(nbins=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = viz.plot_td_table(inputs.table, plot_params=dict(label=\"original\"))\n",
    "viz.plot_td_table(\n",
    "    outputs2.table, plot_params=dict(label=\"modified\"), fig_axes=(fig, ax)\n",
    ")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = viz.plot_warping(outputs.synth_seismic, outputs.seismic, outputs2.dlags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wtie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
